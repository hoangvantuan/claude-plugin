---
name: writer-agent
description: Transform documents into styled article series. Analyze input (md, txt, pdf, docx, pptx, xlsx, html, epub, images, url), extract core ideas, decompose into logical sections, write articles with user-selectable styles (professional, casual, custom), synthesize into organized output. Uses Docling for high-quality document conversion. Handles large documents with hierarchical summarization. Output to docs/generated/.
disable-model-invocation: true
version: 1.16.0
license: MIT
---

# Writer Agent

Transform documents and URLs into styled article series.

## Quick Reference

| Reference                                                             | Purpose                            |
| --------------------------------------------------------------------- | ---------------------------------- |
| [directory-structure.md](references/directory-structure.md)           | Output folder layout               |
| [decision-trees.md](references/decision-trees.md)                     | Workflow decision guides           |
| [retry-workflow.md](references/retry-workflow.md)                     | Error recovery procedures          |
| [large-doc-processing.md](references/large-doc-processing.md)         | Handling documents >50K words      |
| [article-writer-prompt.md](references/article-writer-prompt.md)       | Subagent prompt templates          |
| [context-extractor-prompt.md](references/context-extractor-prompt.md) | Context extraction template        |
| [context-optimization.md](references/context-optimization.md)         | Context optimization anti-patterns |
| [performance-benchmarks.md](references/performance-benchmarks.md)     | Measured performance test cases    |
| [detail-levels.md](references/detail-levels.md)                       | Output detail level options        |

## Workflow Overview

**Direct Path (<20K words OR <50K words with <=3 articles):**

Main agent writes all articles directly without subagents.

```
Input ‚Üí Convert ‚Üí Plan ‚Üí Write(main) ‚Üí Synthesize ‚Üí Verify
  1        1        3         4            5           6
```

**Standard (Tier 1-2, 20K-100K words):**

```
Input ‚Üí Convert ‚Üí Analyze ‚Üí Extract ‚Üí Write ‚Üí Synthesize ‚Üí Verify
  1        1         3         3        4         5           6
```

**Fast Path (Tier 3, >=100K words):**

```
Input ‚Üí Convert ‚Üí Plan ‚Üí Write(parallel) ‚Üí Synthesize ‚Üí Verify
  1        1        3          4              5           6
```

## Step 0: Resolve Skill Paths

**PH·∫¢I ch·∫°y tr∆∞·ªõc m·ªçi b∆∞·ªõc kh√°c.** Resolve ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø c·ªßa skill ƒë·ªÉ t∆∞∆°ng th√≠ch c·∫£ project-local v√† global install.

```bash
# T√¨m wa-env script
WA_ENV=$(find ~/.claude -name wa-env -path "*/writer-agent/*" -type f 2>/dev/null | head -1)
[ -z "$WA_ENV" ] && WA_ENV=$(find .claude -name wa-env -path "*/writer-agent/*" -type f 2>/dev/null | head -1)

# Parse output ‚Üí l·∫•y c√°c bi·∫øn
eval "$($WA_ENV)"
```

**K·∫øt qu·∫£**: C√°c bi·∫øn `SCRIPTS_DIR`, `SKILL_DIR`, `STYLES_DIR` s·∫µn s√†ng d√πng cho c√°c b∆∞·ªõc sau.

**Fallback** (n·∫øu wa-env kh√¥ng t√¨m th·∫•y):

```bash
# T√¨m b·∫•t k·ª≥ wa-convert n√†o
WA_CONVERT=$(find ~/.claude .claude -name wa-convert -path "*/writer-agent/*" -type f 2>/dev/null | head -1)
SCRIPTS_DIR=$(dirname "$WA_CONVERT")
SKILL_DIR=$(dirname "$SCRIPTS_DIR")
STYLES_DIR="$SKILL_DIR/output_styles"
```

> **L∆∞u √Ω**: T·∫•t c·∫£ commands trong c√°c b∆∞·ªõc sau d√πng `$SCRIPTS_DIR`, `$SKILL_DIR`, `$STYLES_DIR` thay v√¨ hardcoded paths.

## Step 1: Input Handling

Detect input type and convert to markdown.

| Input Type               | Detection               | Action                    |
| ------------------------ | ----------------------- | ------------------------- |
| File (PDF/DOCX/EPUB/etc) | Path + extension        | `wa-convert {path}`       |
| URL                      | `http://` or `https://` | `wa-convert {url}`        |
| Plain text / .txt / .md  | No complex extension    | Rewrite ‚Üí `wa-paste-text` |

### File/URL Conversion

```bash
$SCRIPTS_DIR/wa-convert [/path/to/file.pdf or url]
```

**Output**: `docs/generated/{slug}-{timestamp}/input-handling/content.md`

### Plain Text Processing

1. Read content (if file)
2. Rewrite to structured markdown (add headings, preserve content)
3. Propose title
4. Execute:

```bash
echo "{rewritten_content}" | $SCRIPTS_DIR/wa-paste-text - --title "{title}"
```

### Error Handling

| Error              | Action                         |
| ------------------ | ------------------------------ |
| File not found     | Ask for correct path           |
| Unsupported format | Try Docling, confirm with user |
| URL fetch failed   | Report and stop                |
| Empty content      | Warn, confirm before continue  |
| Encrypted PDF      | Ask for decrypted version      |

## Step 2: Select Style

Use `AskUserQuestion` to confirm output style.

| Style                   | File                         | Voice                                |
| ----------------------- | ---------------------------- | ------------------------------------ |
| Professional            | `professional.md`            | Formal, data-driven, 3rd person      |
| Explanatory             | `explanatory.md`             | Teaching, "we" together              |
| Mindful Educator        | `mindful-educator.md`        | Depth + practice + mindfulness       |
| Introspective Narrative | `introspective-narrative.md` | Personal journey, "I"                |
| Mindful Dialogue        | `mindful-dialogue.md`        | Master-student dialogue              |
| Mindful Storytelling    | `mindful-storytelling.md`    | First person storytelling            |
| Deep Dive               | `deep-dive.md`               | Investigative, assumption-challenging |

Style files: `$STYLES_DIR/{style}.md`

## Step 2.5: Select Detail Level

Use `AskUserQuestion` to confirm output detail level.

| Level         | Ratio  | Description                         |
| ------------- | ------ | ----------------------------------- |
| Concise       | 15-25% | T√≥m l∆∞·ª£c, gi·ªØ √Ω ch√≠nh               |
| **Standard**  | 30-40% | C√¢n b·∫±ng (Recommended)              |
| Comprehensive | 50-65% | Chi ti·∫øt, gi·ªØ nhi·ªÅu v√≠ d·ª•           |
| Faithful      | 75-90% | G·∫ßn nh∆∞ ƒë·∫ßy ƒë·ªß, vi·∫øt l·∫°i theo style |

**Default**: Standard (if user skips or unclear)

### Calculate Target Words (Tham kh·∫£o)

**L∆ØU √ù**: Target words ch·ªâ mang t√≠nh tham kh·∫£o. PASS/FAIL d·ª±a tr√™n section coverage, kh√¥ng ph·∫£i word count.

```
target_ratio = midpoint of selected level
total_target = source_words √ó target_ratio

Per article (reference only):
article_target = (article_source_words / source_words) √ó total_target
# Word count ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng, kh√¥ng b·∫Øt bu·ªôc ƒë·∫°t ch√≠nh x√°c
```

### Understanding Detail Level Parameters

**Two complementary concepts:**

1. **`target_ratio`**: Controls total article length relative to source

   * Standard level: 30-40% (midpoint 35%)

   * This ratio applies to the entire article wordcount

2. **`example_percentage`**: Controls retention of examples within kept content

   * Standard level: 60% of examples

   * This percentage applies only to example sections

**Worked example (Standard level, 35% target ratio, 60% examples):**

* Source section: 5,000 words total

  * Main explanatory content: 4,000 words

  * Examples (10 examples): 1,000 words

* Target article length: 5,000 √ó 0.35 = 1,750 words

* Keep 60% of examples: 6 examples ‚âà 600 words

* Remaining budget for main content: 1,750 - 600 = 1,150 words

* Main content compression: 1,150 / 4,000 = 28.75% (summarized)

**Key insight:** Higher `example_percentage` (60%) than overall `target_ratio` (35%) means examples are preserved more than prose, reflecting their teaching value.

See [detail-levels.md](references/detail-levels.md) for full specification.

## Step 2.6: Tier Reference Table

**Canonical tier definitions** (referenced throughout documentation):

| Tier            | Word Count                     | Strategy                       | Context Approach   | Glossary                    | max\_concurrent |
| --------------- | ------------------------------ | ------------------------------ | ------------------ | --------------------------- | --------------- |
| **Direct Path** | <20K OR (<50K AND ‚â§3 articles) | Main agent writes all          | N/A (no subagents) | Inline (\~200 words)        | N/A             |
| **Tier 1**      | 20K-50K                        | Subagents read source directly | No context files   | Inline (\~200 words)        | 3               |
| **Tier 2**      | 50K-100K                       | Smart compression              | Context extractors | Separate file (\~600 words) | 3               |
| **Tier 3**      | >=100K                         | Fast Path, minimal overhead    | No context files   | Inline (\~300 words)        | 2               |

**Priority rules:**

* Direct Path conditions are checked FIRST and override tier boundaries

* Documents 20K-50K with ‚â§3 articles use Direct Path (not Tier 1)

* Only documents failing Direct Path conditions fall through to tier selection

**Key differences:**

* Direct Path: Main agent handles everything (no subagents)

* Tier 1: Lightweight subagents, read source via line ranges

* Tier 2: Context extraction for compression (only tier with separate glossary file)

* Tier 3: Like Tier 1 but larger chunks, more selective glossary, lower concurrency

## Step 3: Analyze

**Goal**: Create analysis artifacts for article generation.

### 3.0 Processing Path Selection

Read `structure.json` ‚Üí use `direct_path` field (computed by `wa-convert` v1.2+):

```
structure.json ‚Üí direct_path.eligible?
‚îú‚îÄ YES AND direct_path.capacity_ok?
‚îÇ   ‚îî‚îÄ DIRECT PATH
‚îÇ       ‚îî‚îÄ Skip context extraction
‚îÇ       ‚îî‚îÄ Main agent writes ALL articles
‚îÇ       ‚îî‚îÄ ~30% faster for small documents
‚îÇ
‚îú‚îÄ YES BUT NOT direct_path.capacity_ok?
‚îÇ   ‚îî‚îÄ WARN: direct_path.warning
‚îÇ   ‚îî‚îÄ RECOMMEND: Use Tier 1 with subagents instead
‚îÇ
‚îú‚îÄ NO AND tier_recommendation.tier <= 2?
‚îÇ   ‚îî‚îÄ STANDARD PATH (3.1-3.5)
‚îÇ
‚îî‚îÄ NO AND tier_recommendation.tier == 3?
    ‚îî‚îÄ FAST PATH (Tier 3)
```

> **Note**: `direct_path` fields in structure.json (since v1.2) include `eligible`, `capacity_ok`, `capacity_limit`, and `warning`. These are pre-computed based on word count, estimated article count, and detected language. Main agent does NOT need to recalculate these values.

**Examples:**

| Document     | Words | Articles | Path            | Reason                                        |
| ------------ | ----- | -------- | --------------- | --------------------------------------------- |
| Blog post    | 15K   | 5        | Direct          | <20K words (first condition) ‚úì                |
| Tutorial     | 45K   | 3        | Direct          | <50K AND ‚â§3 articles (second condition) ‚úì     |
| Long guide   | 48K   | 3        | Direct ‚Üí Tier 1 | Exceeds max\_words for mixed (38K) ‚ö†Ô∏è         |
| Paper        | 45K   | 4        | Standard        | Fails both conditions (4 > 3) ‚Üí use subagents |
| Book chapter | 67K   | 8        | Standard        | Tier 2: smart compression                     |
| Full book    | 142K  | 12       | Fast            | Tier 3: reference-based                       |

> **Note**: Direct Path capacity limit depends on language: EN ~44K, VI ~32K, mixed ~38K words. Use `structure.json ‚Üí language` field for accurate limit.

### 3.1 Structure Scan

> **üìñ READ FIRST**: [context-optimization.md](references/context-optimization.md) explains anti-patterns that waste 50%+ context budget. Review before proceeding.

**Quick path** (if `structure.json` exists):

* **ONLY** read `structure.json` for outline, stats, tier recommendation

* **DO NOT** read `content.md` - it wastes context budget

* Skip manual scanning (outline already in JSON)

**Fallback** (if `structure.json` missing):

‚ö†Ô∏è **WARNING**: Fallback mode loses 51% context optimization. Re-run `wa-convert` to generate `structure.json` if possible.

Manual scan using efficient commands:

```bash
# Extract heading structure without reading full file
grep -n "^#" docs/generated/{slug}/input-handling/content.md | head -100

# Or use line-based sampling (first 100 lines for overview)
Read(file_path, offset=1, limit=100)  # Only to extract headings
```

‚ö†Ô∏è **CRITICAL**: Do NOT read full `content.md` during structure scan! For all tiers, subagents will read source content directly when writing articles. Reading it now wastes 90%+ context budget. See [context-optimization.md](references/context-optimization.md) for budget examples and common mistakes.

### 3.1.1 Tier 3 Fast Path (>=100K words)

For very large documents, minimize analysis overhead:

| Action | Detail                                                            |
| ------ | ----------------------------------------------------------------- |
| SKIP   | `_glossary.md`, context files                                     |
| CREATE | Minimal `_plan.md` (section-to-article mapping + line ranges)     |
| EMBED  | Key terms (\~300 words) + dependencies inline in subagent prompts |
| SPAWN  | Subagents immediately after `_plan.md` (continuous batching)      |

**Context savings**: \~40% reduction in main agent context.

See [large-doc-processing.md](references/large-doc-processing.md#tier-3-fast-path) for `_plan.md` format, subagent prompt template, and workflow details.

### 3.2 Content Inventory

Use `structure.json` outline directly. Section IDs, line ranges, word counts, and critical markers are all available in `structure.json`.

### 3.3 Article Plan (`analysis/_plan.md`)

**Check user request first:**

```python
# Priority: User request > Auto-split
if user_specified_article_count:
    # User y√™u c·∫ßu s·ªë b√†i c·ª• th·ªÉ (v√≠ d·ª•: "chia th√†nh 5 b√†i")
    target_articles = user_specified_count
    skip_auto_split = True
    # Ph√¢n b·ªï sections ƒë·ªÅu cho c√°c b√†i, kh√¥ng chia nh·ªè th√™m
else:
    # Auto mode: chia th√†nh 3-7 b√†i, m·ªói b√†i ~10 ph√∫t ƒë·ªçc
    target_articles = calculate_optimal_articles(total_words, detail_ratio)
    skip_auto_split = False
```

Group sections into articles (default 3-7, or user-specified count):

```markdown
| #   | Slug  | Title         | Sections      | Est. Words | Reading Time |
| --- | ----- | ------------- | ------------- | ---------- | ------------ |
| 1   | intro | Introduction  | S01, S02      | 2000       | ~13 min      |
| 2   | core  | Core Concepts | S03, S04, S05 | 2500       | ~13-15 min   |
```

**Rules**:

* All sections must be mapped. Coverage check at end.

* Target \~13-15 ph√∫t ƒë·ªçc/b√†i (2000-3000 t·ª´)

* N·∫øu user ch·ªâ ƒë·ªãnh s·ªë b√†i ‚Üí tu√¢n theo, kh√¥ng auto-split th√™m

**Content-Type Detection (t·∫°o c√πng l√∫c v·ªõi plan):**

Khi t·∫°o `_plan.md`, x√°c ƒë·ªãnh `content_type` cho m·ªói article:

| Content Type | Suggested Structure | Khi n√†o |
|-------------|-------------------|---------|
| `tutorial` | Problem ‚Üí Solution ‚Üí Steps ‚Üí Practice | H∆∞·ªõng d·∫´n, how-to |
| `conceptual` | Question ‚Üí Exploration ‚Üí Framework ‚Üí Implications | L√Ω thuy·∫øt, tri·∫øt h·ªçc |
| `narrative` | Scene ‚Üí Conflict ‚Üí Journey ‚Üí Resolution | C√¢u chuy·ªán, memoir |
| `analysis` | Finding ‚Üí Evidence ‚Üí Discussion ‚Üí Application | Nghi√™n c·ª©u, report |
| `mixed` | Follow output style's default Structure | N·ªôi dung h·ªón h·ª£p |

Detection signals:

| Signal | Content Type |
|--------|-------------|
| Step-by-step headings, numbered lists, "how to" | `tutorial` |
| Questions as headings, thesis statements, arguments | `conceptual` |
| Narrative structure, characters, timeline | `narrative` |
| Data tables, methodology, findings | `analysis` |
| Mix of above | `mixed` (use dominant) |

Ghi v√†o plan table:

```markdown
| #   | Slug  | Title         | Sections      | Est. Words | Content Type |
| --- | ----- | ------------- | ------------- | ---------- | ------------ |
| 1   | intro | Introduction  | S01, S02      | 2000       | conceptual   |
| 2   | core  | Core Concepts | S03, S04, S05 | 2500       | tutorial     |
```

Subagent s·ª≠ d·ª•ng: Embed `CONTENT_TYPE: {type}` v√†o prompt. Subagent ∆∞u ti√™n:
1. Output style's Structure (primary)
2. Content-type hint (secondary, n·∫øu style kh√¥ng c√≥ structure c·ª• th·ªÉ cho lo·∫°i n√†y)

**Series Context (QUAN TR·ªåNG - t·∫°o c√πng l√∫c v·ªõi plan):**

Khi t·∫°o `_plan.md`, ƒë·ªìng th·ªùi x√°c ƒë·ªãnh:

```markdown
## Series Context

Core message: "{1-2 c√¢u th√¥ng ƒëi·ªáp c·ªët l√µi}"

| # | Title | Role | Reader Enters | Reader Exits | Bridge to Next |
| 1 | Intro | foundation | Ch∆∞a bi·∫øt X | Hi·ªÉu X c∆° b·∫£n | "Nh∆∞ng X trong th·ª±c t·∫ø...?" |
| 2 | Core | development | Hi·ªÉu X c∆° b·∫£n | N·∫Øm v·ªØng Y | "Y m·ªü ra c√¢u h·ªèi v·ªÅ Z..." |
| 3 | Adv | climax | N·∫Øm v·ªØng Y | K·∫øt n·ªëi Y v·ªõi Z | N/A (last) |
```

**C√°ch t·∫°o Reader Enters/Exits/Bridge:**
- `Reader Enters`: Ki·∫øn th·ª©c ng∆∞·ªùi ƒë·ªçc c√≥ khi b·∫Øt ƒë·∫ßu b√†i (t·ª´ b√†i tr∆∞·ªõc ho·∫∑c ki·∫øn th·ª©c n·ªÅn)
- `Reader Exits`: Ki·∫øn th·ª©c ng∆∞·ªùi ƒë·ªçc ƒë·∫°t ƒë∆∞·ª£c sau b√†i (d·∫´n t·ªõi b√†i sau)
- `Bridge to Next`: 1 c√¢u g·ª£i t√≤ m√≤ k·∫øt n·ªëi b√†i n√†y v·ªõi b√†i ti·∫øp (KH√îNG d√πng "Trong ph·∫ßn ti·∫øp theo...")

Th√¥ng tin n√†y s·∫Ω ƒë∆∞·ª£c embed v√†o `SERIES_CONTEXT` block trong m·ªói subagent prompt (xem [article-writer-prompt.md](references/article-writer-prompt.md#series-context-block)).

### 3.3.1 Article Splitting (Auto)

**Trigger**: After Step 3.3, before Step 3.4. Check each planned article.

**Priority rules:**

1. **User-specified count**: N·∫øu user y√™u c·∫ßu s·ªë b√†i c·ª• th·ªÉ ‚Üí tu√¢n theo, KH√îNG auto-split
2. **Auto-split**: Ch·ªâ √°p d·ª•ng khi user KH√îNG y√™u c·∫ßu s·ªë b√†i c·ª• th·ªÉ

**Key constants:**

* `MAX_OUTPUT_WORDS = 3000` (\~15 min reading time)

* `TARGET_PART_WORDS = 2000` (\~13 min reading time)

* Atomic unit = H2 block (H2 + H3 children). NEVER split within paragraph, H3, or critical section.

**When to split**: `estimated_output = source_words √ó detail_ratio > MAX_OUTPUT_WORDS`

**Algorithm**: Greedy grouping of H2 blocks, no minimum. See [large-doc-processing.md#article-splitting-strategy](references/large-doc-processing.md#article-splitting-strategy) for full algorithm and validation matrix.

**Validate after split:**

```bash
$SCRIPTS_DIR/wa-validate-split docs/generated/{book}/analysis/_plan.md
```

**Part naming**: `02-core.md` ‚Üí `02-core-part1.md`, `02-core-part2.md`

**Context bridging**: For Part N > 1, provide prev part topics, last paragraph, key concepts. See [article-writer-prompt.md#multi-part-article-template](references/article-writer-prompt.md#multi-part-article-template).

### 3.4 Shared Context (Inline Glossary)

‚ö†Ô∏è **TIMING**: Execute AFTER Steps 3.1-3.3 complete, BEFORE Step 3.5.

**Strategy by tier:**

```
word_count < 50,000 (Tier 1)?
‚îú‚îÄ YES ‚Üí Extract inline glossary (~200 words) from first ~300 lines
‚îÇ   ‚îî‚îÄ Embed in each subagent prompt
‚îÇ   ‚îî‚îÄ Skip separate _glossary.md
‚îÇ   ‚îî‚îÄ Saves 1 Read call per subagent (~400 words saved)
‚îÇ
‚îú‚îÄ 50,000 <= word_count < 100,000 (Tier 2)?
‚îÇ   ‚îî‚îÄ SKIP inline extraction (context extractors in Step 3.5 will create _glossary.md)
‚îÇ       ‚îî‚îÄ Article writers (Step 4) will read shared _glossary.md file (~600 words)
‚îÇ       ‚îî‚îÄ More comprehensive terminology than inline approach
‚îÇ
‚îî‚îÄ word_count >= 100,000 (Tier 3)?
    ‚îî‚îÄ Extract inline glossary (~300 words) from first ~500 lines
        ‚îî‚îÄ Embed in each subagent prompt
        ‚îî‚îÄ Skip separate _glossary.md
        ‚îî‚îÄ Larger than Tier 1 due to more technical terminology
```

**Extraction algorithm**: See [context-optimization.md#glossary-extraction-algorithm](references/context-optimization.md#glossary-extraction-algorithm) for detailed process.

**Quick process**:

1. Read content.md first 300-500 lines (tier-dependent)
2. Extract terms using definition patterns
3. Score by importance (frequency + position)
4. Take top N terms until hitting word budget
5. Format: `Term: definition (~20 words each)`

**Tier 3 inline rationale:**

* Avoids 1 Read call per subagent (saves \~400 words/subagent)

* Trade-off: More selective terms (300 vs 1000) but faster execution

* Larger than Tier 1 (300 vs 200 words) because large documents have more technical terminology

* Combined with reading source directly via line ranges = maximum efficiency

**Inline glossary format**:

```markdown
## Terms

Term1: definition (~20 words)
Term2: definition
Term3: definition
```

Article dependencies: Embed 1-2 sentences in prompt, not separate file.

### 3.5 Context Files

**Skip for**:

* Tier 1 (<50K words): Subagents read source directly via line ranges

* Tier 3 (>=100K words): Subagents read source directly via line ranges

* Direct Path (<20K words): Main agent writes directly

**Decision** (see [decision-trees.md#3](references/decision-trees.md#3-context-extraction-strategy) for full tree):

* Tier 1/3 or <20K words: Skip context files (subagents read source directly via line ranges)

* Tier 2 (50K-100K): Spawn context extractor subagents (batch: min(3, article\_count))

* Template: `templates/_context-file-template.md`

Each context file: `analysis/XX-{slug}-context.md`

### 3.6 Quality Gate: Analysis Complete

Before proceeding to Step 4, verify:

* [ ] All sections have IDs (from structure.json)

* [ ] Critical sections marked (\* auto-detected in structure.json)
  * **Guideline**: Th∆∞·ªùng <=30% sections l√† critical

  * **If >30%**: T·ª± ƒë·ªông ghi nh·∫≠n trong `_plan.md`, KH√îNG c·∫ßn user confirmation

    * Document: "High critical ratio: {ratio}% - technical content"

    * Ti·∫øp t·ª•c workflow b√¨nh th∆∞·ªùng

  * **If >50%**: T·ª± ƒë·ªông chuy·ªÉn sang Tier 3 strategy (read source directly)

    * KH√îNG c·∫ßn STOP ho·∫∑c ask user

    * Tier 3 x·ª≠ l√Ω ƒë∆∞·ª£c high critical ratio v√¨ ƒë·ªçc source tr·ª±c ti·∫øp

    * Ghi log: "Auto-escalated to Tier 3 due to high critical ratio"

  * **Rationale**: T·ª± ƒë·ªông x·ª≠ l√Ω thay v√¨ blocking workflow ƒë·ªÉ h·ªèi user

* [ ] Article plan covers 100% sections

* [ ] For Tier 3: \_plan.md created with line ranges

## Step 4: Write Articles

### 4.0 State Tracking (Recommended)

For resume and retry support, create/update `analysis/_state.json`. Required if retry-workflow is needed (see [retry-workflow.md](references/retry-workflow.md)):

```json
{
  "status": "in_progress",
  "current_step": 4,
  "completed_articles": ["00-overview.md"],
  "pending_articles": ["01-intro.md", "02-core.md"]
}
```

See [retry-workflow.md](references/retry-workflow.md#state-persistence) for details.

For selective re-runs (style change or single article rewrite), see [retry-workflow.md#selective-re-run](references/retry-workflow.md#selective-re-run).

### 4.1 Overview Article (Phase 1)

Write `00-overview.md` in **main context**:

* Requires full series knowledge

* Template: `templates/_overview-template.md`

* Target: 300-400 words (initial)

* Include placeholders for Key Takeaways and Article Index

**Phase 1 content**:

* Surprising insight + Micro-story + Core questions + Why It Matters

* Placeholder sections for ƒêi·ªÉm ch√≠nh and M·ª•c l·ª•c

### 4.2 Content Articles

**Direct Path** (<20K words): Main agent writes all articles directly.

**Standard/Fast Path**: Spawn subagents for articles 01+:

```
Task tool:
- subagent_type: "general-purpose"
- description: "Write: {title}"
- prompt: [Use references/article-writer-prompt.md]
```

**Multi-Part Articles** (from Step 3.3.1):

For split articles, spawn each part sequentially within the article:

```
# Article 2 was split into 3 parts
1. Spawn 02-core-part1.md
2. Wait for completion ‚Üí extract context bridge
3. Spawn 02-core-part2.md (with context from part1)
4. Wait ‚Üí extract context bridge
5. Spawn 02-core-part3.md (with context from part2)

# Other articles can run in parallel
# e.g., 01-intro.md and 03-advanced.md can run while part2 waits
```

**Context bridge extraction:**

```python
def extract_context_bridge(completed_part):
    """Extract context for next part from completed part"""
    article_content = read(completed_part.output_path)
    return {
        'prevPartTopics': extract_h2_titles(article_content),
        'prevPartEnding': get_last_paragraph(article_content, max_words=50),
        'keyConceptsFromPrev': extract_bold_terms(article_content)
    }
```

**Prompt validation (optional, for debugging):**

```bash
echo "{prompt_text}" | $SCRIPTS_DIR/wa-validate-prompt --tier {1|2|3} --stdin
```

Validates all required template variables are present. Exit code 0 = PASS, 1 = missing variables.

**Continuous Batching** (preferred over static batching):

* Tier 1-2: `max_concurrent = 3` (smaller chunks \~3.5K words)

* Tier 3: `max_concurrent = 2` (larger chunks \~10K words)

* Dynamic adjustment: large chunks (>8K) ‚Üí reduce to 2, all small (<2K) ‚Üí increase to 5

* On any completion ‚Üí spawn next immediately (no batch waiting)

* **Benefits**: 25-35% faster than static batching

See [large-doc-processing.md#continuous-batching-vs-static](references/large-doc-processing.md#continuous-batching-vs-static) for full algorithm and [performance-benchmarks.md](references/performance-benchmarks.md) for benchmarks.

**Progress Reporting**:

After each article completes, update TaskUpdate:

* Format: `"Writing articles: {completed}/{total} completed"`

* Example: `"Writing articles: 3/7 completed"`

* Do NOT include time estimates

### 4.3 SoT Pattern (Long Articles)

**When to use** Skeleton-of-Thought: estimated output >2000 words AND >=5 subsections (H3 preferred, fallback to H2).

**Quick decision**: `h3_count >= 5` ‚Üí SoT. `h3 == 0 AND h2 >= 5` ‚Üí SoT. Otherwise ‚Üí standard write.

**Workflow**: Phase 1 (skeleton) ‚Üí Phase 2 (expand ALL sections parallel) ‚Üí Phase 3 (merge + transitions)

**Benefits**: 45-50% faster for long articles. See [article-writer-prompt.md#sot-pattern](references/article-writer-prompt.md#sot-pattern) for template and [performance-benchmarks.md#test-case-5](references/performance-benchmarks.md#test-case-5-sot-pattern-long-article) for benchmarks.

**Limitations**: Priority 3 (paragraph breaks) not implemented. Ambiguous structure ‚Üí default to standard write.

### 4.4 Coverage Tracking

Subagent reports coverage in **return message** (not in article file) using **table format**. See [Step 5.2](#52-coverage-aggregation) for aggregation details.

**IMPORTANT**: PASS/FAIL ch·ªâ d·ª±a tr√™n section coverage, kh√¥ng ph·∫£i word count. Word count ch·ªâ mang t√≠nh th·ªëng k√™.

**Subagent return format** (2-column, see article-writer-prompt.md):

```markdown
DONE: {filename} | {N} words (stats)
COVERAGE (determines PASS/FAIL):
| Section | Status |
|---------|--------|
| S01 | ‚úÖ quoted |
| S02 ‚≠ê | ‚úÖ faithful |
RESULT: PASS # PASS n·∫øu all sections covered
```

**Ti√™u ch√≠ PASS/FAIL:**

* **PASS**: T·∫•t c·∫£ sections ƒë∆∞·ª£c assigned ƒë·ªÅu ƒë∆∞·ª£c covered (100% section coverage)

* **FAIL**: C√≥ section b·ªã missing ho·∫∑c skipped kh√¥ng h·ª£p l·ªá

* **Word count**: Ch·ªâ th·ªëng k√™, KH√îNG ·∫£nh h∆∞·ªüng PASS/FAIL

Main agent enriches with "Assigned To" and "Used In" columns ‚Üí aggregates into `_coverage.md` (4-column format, see [Step 5.2](#52-coverage-aggregation)).

### 4.5 Critical Sections

**‚≠ê sections MUST be faithfully rewritten** (kh√¥ng t√≥m t·∫Øt, kh√¥ng b·ªè √Ω):

* Gi·ªØ 100% √Ω nghƒ©a v√† th√¥ng tin g·ªëc ‚Äî KH√îNG ƒë∆∞·ª£c t√≥m t·∫Øt hay l∆∞·ª£c b·ªè

* PH·∫¢I vi·∫øt l·∫°i b·∫±ng ti·∫øng Vi·ªát theo voice c·ªßa output style ƒë√£ ch·ªçn

* KH√îNG copy nguy√™n vƒÉn t·ª´ source

* If unable to include fully ‚Üí flag for review

### 4.6 Quality Gate: Articles Complete

Before proceeding to Step 5, verify:

* [ ] All articles written (check pending list)

* [ ] **Each article has "## C√°c b√†i vi·∫øt trong series" at end** (check `SERIES_LIST: YES` in subagent return)
  * If `SERIES_LIST: NO` ‚Üí Append series list to article file before continuing

* [ ] Coverage reports collected from all subagents

* [ ] No placeholder text in articles

* [ ] Source verification quotes provided

* [ ] Opening of each article is NOT mechanical ("Trong b√†i n√†y...")

## Step 5: Synthesize

### 5.1 Update Overview (Phase 2)

Update `00-overview.md` with actual content for placeholder sections:

**ƒêi·ªÉm ch√≠nh** (Key Takeaways):

```markdown
## ƒêi·ªÉm ch√≠nh

1. **[Concept 1]**: [Brief explanation from series]
2. **[Concept 2]**: [Brief explanation from series]
3. **[Concept 3]**: [Brief explanation from series]
```

**C√°c b√†i vi·∫øt trong series** (Series List):

```markdown
## C√°c b√†i vi·∫øt trong series

1. **T·ªïng quan - Brief description** _(ƒëang xem)_
2. [Article 1 Title](./01-slug.md) - Brief description
3. [Article 2 Title](./02-slug.md) - Brief description
```

**Final overview target**: 400-600 words

### 5.2 Coverage Aggregation

Collect subagent coverage tables ‚Üí aggregate into `analysis/_coverage.md`

**Process**:

1. Each subagent returns a 2-column COVERAGE TABLE (`| Section | Status |`) in their return message
2. Main agent enriches each row with "Assigned To" and "Used In" columns (from `_plan.md`)
3. Concatenate all enriched tables into single `_coverage.md` file (4-column format)
4. Add summary statistics at bottom

**Column enrichment**: Main agent knows which article each subagent wrote, so it adds:

* `Assigned To`: article filename (from `_plan.md`)

* `Used In`: same as Assigned To (or different if reassigned during writing)

**Coverage file format** (required by `validate_coverage.py`):

```markdown
## Section Coverage Matrix

| Section | Assigned To   | Used In       | Status        |
| ------- | ------------- | ------------- | ------------- |
| S01     | 01-article.md | 01-article.md | ‚úÖ summarized |
| S02 ‚≠ê  | 01-article.md | 01-article.md | ‚úÖ faithful   |

- Total: {N} | Used: {N} | Missing: {N}
```

**Format rules**:

* Column 1: `S{NN}` with optional `‚≠ê` for critical sections

* Column 4:

  * For used sections: `‚úÖ` followed by one of: `used`, `faithful`, `quoted`, `summarized`

  * For skipped sections: `‚ö†Ô∏è skipped` (requires Notes column with reason)

* Summary line at bottom: `- Total: {N} | Used: {N} | Missing: {N}`

**Edge case examples**:

```markdown
| Section | Assigned To | Used In               | Status        | Notes                      |
| ------- | ----------- | --------------------- | ------------- | -------------------------- |
| S05     | 01-intro.md | 02-core.md            | ‚úÖ summarized | Reassigned during planning |
| S08     | 02-core.md  | 02-core.md, 03-adv.md | ‚úÖ quoted     | Shared across articles     |
| S12     | 03-adv.md   | -                     | ‚ö†Ô∏è skipped    | Redundant with S08         |
```

**Multi-Part Article Coverage:** For split articles, track by part in `_coverage.md`. Each section row should sum to ~100%. See [large-doc-processing.md#coverage-tracking](references/large-doc-processing.md#coverage-tracking) for format and validation rules.

**Edge case rules:**

1. **Reassignment**: Section moved to different article (common when planning adjusts)

   * "Assigned To" shows original plan

   * "Used In" shows actual article that included it

   * Validate coverage in "Used In" article

2. **Shared sections** (one section used in multiple articles):

   * Format: `Used In` = comma-separated list (e.g., `02-core.md, 03-adv.md`)

   * Validation: Each article in the list MUST contain \[Sxx] reference

   * Check both articles include the section (quoted, summarized, or paraphrased)

   * Status reflects how primary article used it

3. **Skipped**: Must document reason (redundant, off-topic, user instruction)

   * Status: `‚ö†Ô∏è skipped` (not `‚úÖ`)

   * Notes column required with explicit reason

Run validation:

```bash
$SCRIPTS_DIR/wa-validate docs/generated/{book}/analysis/_coverage.md
```

## Step 6: Verify

### 6.1 Coverage Check

**Soft target**: Coverage n√™n ƒë·∫°t >=95% (kh√¥ng b·∫Øt bu·ªôc retry)

```
Coverage results:
‚îú‚îÄ >= 95% ‚Üí PASS (ti·∫øp t·ª•c)
‚îú‚îÄ 90-94% ‚Üí WARNING (ghi nh·∫≠n, kh√¥ng retry t·ª± ƒë·ªông)
‚îÇ   ‚îî‚îÄ Ch·ªâ retry n·∫øu user y√™u c·∫ßu
‚îú‚îÄ < 90% ‚Üí ASK USER
‚îÇ   ‚îî‚îÄ Option 1: Accept as-is
‚îÇ   ‚îî‚îÄ Option 2: Retry specific articles
‚îÇ   ‚îî‚îÄ Option 3: Create supplementary
```

**QUAN TR·ªåNG**: Kh√¥ng t·ª± ƒë·ªông retry ƒë·ªÉ ƒë·∫°t coverage target. Vi·ªác retry t·ªën token v√† th·ªùi gian, th∆∞·ªùng kh√¥ng c·∫£i thi·ªán ƒë√°ng k·ªÉ.

### 6.2 Quality Checklist

* [ ] All articles written, reader-ready (no metadata)

* [ ] Overview updated with Key Takeaways and Series List

* [ ] **All articles have "## C√°c b√†i vi·∫øt trong series" at the end** (MANDATORY)

* [ ] All links in series lists verified

* [ ] \_coverage.md reported (>=95% target, >=90% acceptable)

* [ ] Critical ‚≠ê sections included (faithful rewrite ‚Äî 100% meaning, Vietnamese, style voice)

* [ ] Warnings logged for any skipped sections

* [ ] **No mechanical openings** ("Trong b√†i n√†y...", "B√†i vi·∫øt s·∫Ω tr√¨nh b√†y...")

* [ ] **No mechanical closings** ("T√≥m l·∫°i, b√†i vi·∫øt ƒë√£...", "Trong ph·∫ßn ti·∫øp theo...")

### 6.3 Error Recovery (User-Driven)

| Error               | Action                         | Auto-retry? |
| ------------------- | ------------------------------ | ----------- |
| Subagent timeout    | Report to user, ask what to do | ‚ùå NO        |
| Missing output      | Log warning, continue          | ‚ùå NO        |
| Style mismatch      | Report, user decides           | ‚ùå NO        |
| Content fabrication | Flag for user review           | ‚ùå NO        |
| Coverage < 90%      | Ask user for decision          | ‚ùå NO        |

**Nguy√™n t·∫Øc**: Kh√¥ng t·ª± ƒë·ªông retry. User c√≥ to√†n quy·ªÅn quy·∫øt ƒë·ªãnh.

See [retry-workflow.md](references/retry-workflow.md) for user decision flow.

## Content Guidelines

### Source Fidelity

* Use ONLY source material, no fabrication

* **REWRITE ALL content in output style voice** ‚Äî Source defines WHAT to say, Style defines HOW to say it

* DO NOT copy-paste sentences from source (bao g·ªìm c·∫£ ‚≠ê critical sections)

* Maintain original terminology (thu·∫≠t ng·ªØ gi·ªØ nguy√™n, nh∆∞ng c√¢u vƒÉn ph·∫£i ƒë∆∞·ª£c vi·∫øt l·∫°i)

* ‚≠ê Critical sections: faithful rewrite ‚Äî gi·ªØ 100% √Ω nghƒ©a, KH√îNG t√≥m t·∫Øt, vi·∫øt l·∫°i b·∫±ng ti·∫øng Vi·ªát + style voice

* Non-critical sections: MUST be rewritten in the selected output style's voice, structure, and language patterns

* VERIFY quotes prove source origin, but article content must be rewritten (not copied)

### Writing Quality

**Narrative Coherence:**

* M·ªói b√†i vi·∫øt ph·∫£i c√≥ m·∫°ch logic ri√™ng, KH√îNG ph·∫£i t√≥m t·∫Øt tu·∫ßn t·ª± t·ª´ng section

* Sections ph·∫£i n·ªëi v·ªõi nhau b·∫±ng bridges (logical ho·∫∑c emotional), kh√¥ng ph·∫£i "Ti·∫øp theo..."

* Draw connections gi·ªØa c√°c √Ω trong b√†i V√Ä v·ªõi th√¥ng ƒëi·ªáp c·ªët l√µi c·ªßa series

**Opening & Closing (quy·∫øt ƒë·ªãnh ·∫•n t∆∞·ª£ng):**

* Opening: Hook compelling (c√¢u h·ªèi, h√¨nh ·∫£nh, kho·∫£nh kh·∫Øc). TR√ÅNH: "Trong b√†i n√†y ch√∫ng ta s·∫Ω..."

* Closing: K·∫øt resonant (c√¢u h·ªèi m·ªü, h√¨nh ·∫£nh, l·ªùi m·ªùi). TR√ÅNH: "T√≥m l·∫°i, b√†i vi·∫øt ƒë√£ tr√¨nh b√†y..."

* Mechanical phrases BLACKLIST: "Trong ph·∫ßn ti·∫øp theo", "Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p ·ªü tr√™n", "B√†i vi·∫øt n√†y s·∫Ω", "T√≥m l·∫°i"

**Depth vs Breadth:**

* Khi m·ªôt √Ω quan tr·ªçng: ƒëi S√ÇU (v√≠ d·ª•, implications, c√¢u h·ªèi) thay v√¨ li·ªát k√™

* Khi nhi·ªÅu √Ω nh·ªè: nh√≥m l·∫°i th√†nh pattern/theme, kh√¥ng li·ªát k√™ t·ª´ng √Ω ri√™ng l·∫ª

* Priority: 2-3 key insights explored deeply > 10 points listed superficially

**Reader Engagement:**

* ƒê·∫∑t c√¢u h·ªèi cho ng∆∞·ªùi ƒë·ªçc (rhetorical ho·∫∑c reflective)

* D√πng v√≠ d·ª• c·ª• th·ªÉ, relatable thay v√¨ abstract

* T·∫°o tension/curiosity tr∆∞·ªõc khi gi·∫£i ƒë√°p

* Vary sentence length: xen k·∫Ω c√¢u ng·∫Øn v√† d√†i

### Formatting

* Link between articles with relative paths

* Track all sections with \[Sxx] IDs

* NO markdown tables in article output - use bullet points instead

* NO diagrams (mermaid, ASCII, flowcharts) - describe in prose or bullets

### Series List (MANDATORY)

* **M·ªñI b√†i vi·∫øt PH·∫¢I c√≥ "## C√°c b√†i vi·∫øt trong series" ·ªü cu·ªëi** - Thi·∫øu = FAIL

* Mark current article with _(ƒëang xem)_

* Validation: Subagent return format includes `SERIES_LIST: YES/NO`

* Main agent MUST check `SERIES_LIST: YES` tr∆∞·ªõc khi accept article

## C√†i ƒë·∫∑t th∆∞ vi·ªán m·ªõi

Skill s·ª≠ d·ª•ng virtual environment t·∫°i `$SCRIPTS_DIR/.venv`. Khi c·∫ßn c√†i th√™m th∆∞ vi·ªán, **PH·∫¢I activate venv tr∆∞·ªõc**:

```bash
# 1. Activate venv (d√πng SCRIPTS_DIR t·ª´ Step 0)
source $SCRIPTS_DIR/.venv/bin/activate

# 2. C√†i package
uv pip install <package>

# 3. C·∫≠p nh·∫≠t requirements.txt
uv pip freeze > $SCRIPTS_DIR/requirements.txt
```

**KH√îNG d√πng:**

* `uv pip install <package>` khi ch∆∞a activate venv ‚Üí l·ªói "No virtual environment found"

* `uv pip install <package> --system` ‚Üí l·ªói "externally managed" (Python Homebrew)

* `uv add <package>` ‚Üí c·∫ßn pyproject.toml, skill d√πng requirements.txt

